\documentclass{article}
\usepackage[linesnumbered,lined,ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\definecolor{epurple}{rgb}{0.58,0,0.33}
\definecolor{eblue}{rgb}{0.31,0.45,1}

\lstdefinestyle{numbers} {numbers=left, stepnumber=1, numberstyle=\tiny, numbersep=10pt}
\lstdefinestyle{defs}{stringstyle=\color{eblue},keywordstyle=\color{epurple}}
\lstdefinestyle{MyJavaStyle} {language=Java,style=numbers,style=defs,frame=trBL,breaklines=true}
\lstdefinestyle{MyXMLStyle} {language=XML,style=numbers,style=defs,frame=trBL,breaklines=true}
\lstdefinestyle{MyPythonStyle} {language=Python,style=numbers,style=defs,frame=trBL,breaklines=true}

\graphicspath{ {img/} }

\title{Decision Tree and Parallel Algorithm on Hadoop}
\author{Md. Nazmul Alam (40016332), Newman ()}

\renewcommand\lstlistingname{Code Snippet}
\renewcommand\lstlistlistingname{Code Snippets}

\begin{document}
\maketitle
\newpage
\tableofcontents
\listofalgorithms
\lstlistoflistings
\addtocontents{toc}{~\hfill\textbf{Page}\par}
\newpage
\section{Introduction}
Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning\cite{wikidt}. However running decision tree on large dataset sequentially takes a lot of time. So, the natural evolution to optimize the sequential algorithm is to introduce parallel decision tree algorithm. Unlike sequential algorithm, parallel algorithm sometimes often tied to the parallel architecture on which it runs. Again, parallel architectures are not easily exposed or available to the algorithm itself. Hadoop implements map-reduce paradigm on commodity hardware in a distributed environment. The map-reduce paradigm reduces the complexity of programming and coordination of each node separately.
\BlankLine This report explores an algorithm for decision tree, which is implemented on hadoop, looks into the performance and covers some analysis of it. Following section describes the organization of this report.\BlankLine

Firstly the report covers about the map-reduce paradigm itself and how it works. This will give the reader a sense how an algorithm, intended to be implemented on a map-reduce environment, should be approached.

Next section will cover a simplified overview of the hadoop architecture and how to set up a two node cluster.

Next two sections will cover map-reduce version of word counting and matrix multiplication algorithms to enable the reader to visualize how an algorithm works in the map-reduce paradigm.

In the next section the report focuses on the main problem, decision tree. The report goes through the problem statement, current state of art and tries to give an overview of the problem. Then the report will look into the algorithm and how it works. Finally it provides the algorithms and code snippets along with the results for empirical studies of the algorithm in future.
In the closing sections, we will briefly go over possible future works and then draw the conclusion.

\section{Map Reduce}
Map reduce paradigm was first published in 2004 by Google’s white paper. The idea behind it was to distribute large set of data to multiple workers, whom will work in parallel. After their work all the output are gathered and then another round of operation is performed to generate the final output. This kind of idea generally seems to be divide and conquer. Where large set of data is broken down and passed to initial workers. The operation performed by the initial worker is called map function, and trivially the workers are termed mapper. All the node executes the same mapper on a subset of the dataset and generate intermediate output. Then the output is grouped and passed to another set of worker. The operation performed by these worker are called reduce function, and the workers are termed reducer. So, from a high-level mapreduce paradigm has 3 steps - 
\begin{itemize}
\item Map
\item Combine/Shuffle
\item Reduce
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{mr-hadoop-overview}
	\caption{Map Reduce Paradigm}
	\label{fig:mrp}
\end{figure}
To be highly distributed, there is no state kept between mapper or reducer. Hence map-reduce model was designed to works only with {key, value} pair data. So, in a typical map-reduce program it takes in input data as {key,value} pair, perform map function to produce intermediate results containing a new {key, value} pairs, group the results from each mapper into {key, list{values}} and passes this new input to the reducer. Once the reducer performs the reduce operation a new {key,value} pair will be generated as output. Generally a programmer is concerned with the implementation of map and reduce function and let the default shuffle/combine operation to take place. However, generally implementation of this paradigm also supports different Combine/Shuffle operation for complex requirements.
In the next section we discuss about an open source implementation of the map-reduce paradigm named Apache Hadoop.


\section{Hadoop Architecture}
\subsection {Overview}
Hadoop is an open source implementation of the map-reduce paradigm. It is written in java. It generally supports any map/reduce implementation which runs on JVM (e,g java, scala), however it can also support other language implementation via streaming api. From a very high level, the main components of hapoods are -
\begin{itemize}
\item HDFS
\item Task Manager
\item Job Tracker
\item Name node
\item Data Node
\end{itemize}

The overall architecture is shown in figure \ref{fig:hadoop-arch}\cite{hadoopibm}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{hadoop-arch}
	\caption{Hadoop Architecture}
	\label{fig:hadoop-arch}
\end{figure}

\subsection {Basic Architecture}

HDFS is the Hadoop Distributed File System, which makes sure the data replication on each of Datanode. So, any input or output written to HDFS by one node is available to any other node within that cluster. This abstracts the distributed programming complexities and let the programmer focus on actual algorithm.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{hadoop-hdfs}
	\caption{Map Reduce Paradigm}
	\label{fig:hdfs}
\end{figure}

Task Manager in the hadoop framework is responsible for distributing the map-reduce application to appropriate data node. More generally speaking it’s main task is to distribute the map-reduce task in each of the node and track it’s status.

Job Tracker generally runs in datanode. The main functionality of it to run the assigned map-reduce task in the node and report back it’s status to Task manager.

Name node component defines the master node within the cluster. So, in the whole cluster there will be a single namenode.

Data node component defines the slave nodes within the cluster. Each processing node will have a component as datanode.

A very high level relationship between these component is shown in figure \ref{fig:hdfs} \cite{c452017}.

\subsection{Setting Up Hadoop Cluster}
This section provides a high level instruction on how to set up a hadoop cluster on linux system. At the time of writing this report, the stable version of apache hadoop was 2.7.3 \cite{hadoopap}. 

Hadoops binary distributions are pretty much self contained. The only dependencies hadoop have are Java SDK and SSH-server. So, in linux system one should install Java SDK (openjdk), ssh-server (e,g- openssh-server) and ssh-client (generally preinstalled). After downloading the binary distribution, it should be extracted and placed on the desired location (e,g /usr/local/hadoop). It is not required but recommended to create an user account (e,g - hduser) to sandbox the access of hadoop. In next section it is provided how to setup a single node hadoop. Once the individual single node are setup a simple configuration will easily link them to make a two node setup.

\subsubsection {Single Node Setup}
Following instruction should be completed to enable single node hadoop setup on a system -
\begin{itemize}
\item Generate ssh public/private key pair. Install the key on hduser account to enable auto login without any password.
\item Add HADOOP\_DIR, JAVA\_HOME in the environment variable
\item Edit \$\{HADOOP\_DIR\}/etc/hadoop/core-site.xml and add the following -\BlankLine
\begin{lstlisting}[caption={Hadoop CoreSite Config},label={lst:coresite},style=MyXMLStyle]
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://master:9000</value>
</property>
\end{lstlisting}
\BlankLine Here the master is the IP address of the master node.
\item Edit \$\{HADOOP\_DIR\}/etc/hadoop/hdfs-site.xml and add the following -\BlankLine
\begin{lstlisting}[caption={Hadoop HDFS Config},label={lst:hdfssite},style=MyXMLStyle]
<property>
 <name>dfs.replication</name>
 <value>2</value>
</property>
<property>
 <name>dfs.permission</name>
 <value>false</value>
</property>
<property>
 <name>dfs.namenode.name.dir</name>
 <value>/usr/local/hadoop/hadoop_dir/namenode</value>
</property>
<property>
 <name>dfs.datanode.data.dir</name>
 <value>/usr/local/hadoop/hadoop_dir/datanode</value>
</property>
\end{lstlisting}
\BlankLine The value for name.dir and data.dir are used for namenode and datanode respectively. So, it must be ensured that these directory existed.

\item Copy the \$\{HADOOP\_DIR\}/etc/hadoop/mapred-site.xml.template to \$\{HADOOP\_DIR\}/etc/hadoop/mapred-site.xml and add the following - \BlankLine
\begin{lstlisting}[caption={Hadoop MapRed Config},label={lst:mapredsite},style=MyXMLStyle]
<property>
 <name>mapreduce.framework.name</name>
 <value>yarn</value>
</property>
\end{lstlisting}
\item Format the namenode using hdfs tool provided in the hadoop distribution
\end{itemize}

At this point single node setup is complete, to start hadoop issue start-dfs.sh and start-yarn.sh subsequently.

\subsubsection{Multi Node Setup}
Following instruction should be completed to enable multi node hadoop setup -
\begin{itemize}
\item Add master ssh-pubkey to the trusted ssh-keys in the slaves. This enables autologin without any password.
\item Make sure each node can run in single node setup
\item Create a new file in the master node  \$\{HADOOP\_DIR\}/etc/hadoop/slaves and enter all the slave IPs (including master if it is also a datanode)
\end{itemize}

At this point multi node setup is complete, to start hadoop issue start-dfs.sh and start-yarn.sh on the master to enable auto start of every slave as datanode.

\section{Word count}
\subsection{Problem Overview}
Word count problem is the introductory program in map-reduce paradigm because it is very easy to parallelize and gives a very clear overview how it works.
\BlankLine The problem word count can be defined as to count the repetation of each word within a file. If the dataset is very large then the sequential algorithm will take much time to compute the counting as it will go through the dataset in O(N) where N is the number of words within that dataset.
\BlankLine
To use a parallel algorithm for it in map-reduce, it is important to think about how the input \{key,value\} pair should be prepared, what mapper should produce and how the reducer will calculate the final result.
\BlankLine
The input for this trivial problem does not need to be converted to anything special, in fact the dataset used in sequential algorithm can be used as it is. This is because the dataset is automatically partitioned by the hadoop framework and passed as \{row\_id, split\_of\_dataset\} to each mapper.
\BlankLine
In the mapper function it is sufficient to produce each word as key and generate a value of 1. This is because using any word as the key, the count of the key will be always 1. 
\BlankLine After each mapper completes its output, the shuffle/combine step will gather all the values with same key, group them and pass them to reducer.
\BlankLine
So, if a word, for example ``HELLO'' appeared 3 times in 3 mappers, each of them giving output as \{HELLO, 1\}. Then the reducer will have \{HELLO, list\{1, 1, 1\}\}. Now in each reducer, it just counts the number of 1 from the value and produces the final output as \{key, value\}. For example given above it will produce \{HELLO, 3\}.

\BlankLine In next section the algorithm is defined formally.

\subsection{Algorithm}
For mapper, the algorithm \ref{algo:wcmap} emits the word and count 1. In the reducer, the algorithm \ref{algo:wcred} emits the word and sum of the count.
\BlankLine
\IncMargin{1em}
\begin{algorithm}
\DontPrintSemicolon
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwFunction{emit}{emit}
\Input{(row\_id, text\_split)}
\Output{(word, 1)}
\BlankLine
\ForEach {$word\in text\_split$}{
\emit(word, 1)
}
\caption{Word Count Mapper\label{algo:wcmap}}
\end{algorithm}
\DecMargin{1em}

\IncMargin{1em}
\begin{algorithm}
\DontPrintSemicolon
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwFunction{emit}{emit}
\Input{(word, list(count))}
\Output{(word, sum)}
\BlankLine
$sum\leftarrow $0\;
\ForEach {$count\in list(count)$}{
	$sum\leftarrow $sum $+$ $count$
}
\emit(word, sum)\;
\caption{Word Count Reducer\label{algo:wcred}}
\end{algorithm}
\DecMargin{1em}

\subsection{Source Code}
\subsection{results}

\section{Matrix multiplication}

\subsection{Problem Overview}
Matrix multiplication is a very common but highly practical operation. The problem of matrix multiplication is believed to be first formualized in 19th centuray, however proof does exists that it was being used even before that. There are many practical usage of large matrix multiplication, for example - simulation of galaxy systems, data analysis/mining, solving equations and many more.

Matrix multiplication can be defined as the multiplication of two matrix ${A_{P*Q},B_{Q*R}}$ to generate resultant matrix ${C_{P*R}}$ where ${C_{(i,j)}}$ is defined as  
\begin{equation}
C_{(i,j)}\ =\ \sum_{k = 1}^{Q} A_{i, k} *  B_{k,j}
\end{equation}

In order to parallelize the matrix multiplication on a map-reduce paradigm, it is required to figure out -
\begin{itemize}
\item The input format 
\item The intermediate output from mapper
\item How reducer will make the final result
\end{itemize}

From \(1\) it is evident that for to calculate the elements in a single row of the resultant matrix C, all of the values from the same row of A is required and all the values from the same column of B is required. In other words, to calculate the result for any ${C_{(i,j)}}$ the value ${A_{(i,0)} ...  A_{(i,R)}}$ and ${B_{(0,j)} ... B_{(P,j)}}$  must be passed to a reducer. This simple analysis provides hints on the mappers ouput.

The map function needs to output for each value within a ${row_{x}}$ in A as ${I_{(x, 0)}, I_{(x, 1)} …  I_{(x,R)}}$ and for each value within a ${col_{y}}$ in B it needs to produce ${I_{(0, y)}, I_{(1, y)} …  I_{(P,y)}}$. All the ${I}$ values with same key will be  collected in the Shuffle/Combine step of map reduce. This ensures that a particular reducer will have all the values to compute 1 for a particular i,j.

The input of a particular matrix is given as ${(MatrixId,i,j,Value)}$ for each value of that matrix.

\subsection{Algorithm}
\IncMargin{1em}
\begin{algorithm}[H]
\DontPrintSemicolon
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwFunction{emit}{emit}
\SetKwFunction{GetColumnSize}{GetColumnSize}
\SetKwFunction{GetRowSize}{GetRowSize}
\Input{(row\_id, (matrix\_id, i, j, value))}
\Output{((a,b), (matrix\_id,j,value))}
\BlankLine
$Q\leftarrow \GetColumnSize($B)\;
$P\leftarrow \GetRowSize($A)\;
	\If{$matrix\_id\ equals\ $A}{
		\For {$k\leftarrow\ $0$\ \KwTo\ $Q}{
			\emit((i,k), (matrix\_id,j,value))
		}
	}
	\ElseIf{$matrix\_id\ equals\ $B}{
		\For {$k\leftarrow\ $0$\ \KwTo $P}{
			\emit((k,j), (matrix\_id,i,value))
		}
	}
\caption{Matrix Mul Mapper\label{IR}}
\end{algorithm}
\DecMargin{1em}

\IncMargin{1em}
\begin{algorithm}[H]
\DontPrintSemicolon
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwFunction{emit}{emit}
\SetKwFunction{GetColumnSize}{GetColumnSize}
\Input{(a,b), list(matrix\_id, i, value)}
\Output{((i,j), value)}
\BlankLine
$Q\leftarrow\ \GetColumnSize($A)\;
sort\ list\ into\ M\ or\ N\ based\ on\ i\;
$sum\leftarrow\ $0\;
\For{$K\leftarrow\ $0$\ \KwTo\ $Q} {
	$m_{ik}\leftarrow\ $0\;
	$n_{kj}\leftarrow\ $0\;
	\If{there\ is\ a\ value\ $K\ for\ $M} {
		$m_{ik}\leftarrow\ $M\big[K\big]
	}
	\If{there\ is\ a\ value\ $K\ for\ $N} {
		$n_{kj}\leftarrow\ $N\big[K\big]
	}
	$sum\leftarrow\ $sum $+$ ($m_{ik}$ $*$ $n_{kj}$)
}

\emit((i,k), sum)

\caption{Matrix Mul Reducer\label{IR}}
\end{algorithm}
\DecMargin{1em}

\subsection{Code Snippet}

The Listing \ref{lst:mmmap} shows the java implements of the mapper.

\begin{lstlisting}[caption={Matrix multiplication mapper code snippet},label={lst:mmmap},style=MyJavaStyle]
protected void map(LongWritable key, Text value, Context context)
	throws IOException, InterruptedException {
	String [] values = value.toString().split(",");
	int lim_k = Integer.parseInt(context.getConfiguration().get("K"));
	int lim_m = Integer.parseInt(context.getConfiguration().get("M"));
	if (values[0].compareTo("M") == 0) {
		for(int k=0; k<lim_k; k++) {
			Text outputKey = new Text();
			Text outputValue = new Text();
			// key = i,k
			outputKey.set(values[1] + "," + k);
			// value = M,j,M(ij)
			outputValue.set("M," + values[2] + "," + values[3]);
			context.write(outputKey, outputValue);
		}
	} else {
		for(int k=0; k<lim_m; k++) {
			Text outputKey = new Text();
			Text outputValue = new Text();
			// key = k,j
			outputKey.set(k + "," + values[2]);
			// values = N,i,N(ij)
			outputValue.set("N," + values[1] + "," + values[3]);
			context.write(outputKey, outputValue);
		}
	}
}
\end{lstlisting}

Listing \ref{lst:mmred} shows the java implemenentation of the reducer.

\begin{lstlisting}[caption={Matrix multiplication reducer code snippet},label={lst:mmred},style=MyJavaStyle]
protected void reduce(Text arg0, Iterable<Text> arg1, Context arg2) throws java.io.IOException ,InterruptedException {
    String [] values;
    HashMap<Integer, Float> M = new HashMap<Integer, Float>();
    HashMap<Integer, Float> N = new HashMap<Integer, Float>();
    int lim_k = Integer.parseInt(arg2.getConfiguration().get("K"));
    for (Text val : arg1) {
        values = val.toString().split(",");
        if (values[0].compareTo("M") == 0) {
            M.put(Integer.parseInt(values[1]), Float.parseFloat(values[2]));
        } else {
            N.put(Integer.parseInt(values[1]), Float.parseFloat(values[2]));
        }
    }
		
    float result = 0.0f;
    float m_ik, n_kj;
    for(int k=0; k < lim_k; k++) {
        m_ik = M.containsKey(k) ? M.get(k) : 0.0f;
        n_kj = N.containsKey(k) ? N.get(k) : 0.0f;
        result += m_ik * n_kj;
    }
		
    arg2.write(null, new Text(arg0.toString() + "," + String.valueOf(result)));
}
\end{lstlisting}

\subsection{Analysis}
Let, A and B are the same $(N,N)$ matrix.
The algorithm described above has a mapper time complexity and a reducer time complexity.
The time complexity of mapper is ${O(N)}$. This is because for each of the value, it has to produce N times of that value.
The time complexity of reducer is ${O(N)}$. This is because it iterates through N values of generated items from M and N and produces the final sum.
So, the overall time complexity of the parallel aglorithm is ${O(N)}$\BlankLine

Parallel Algorithm Time, ${T_{p}}$ = ${O(N)}$

Naive Sequential Algorithm, ${T_{s}}$ = ${O(N^3)}$

Number of processor, p = ${N^2}$, as there are ${N^2}$ values for a particular matrix.

Speedup, ${S = T_{s}/T_{p}}$ = ${O(N^3)/O(N)}$ = ${O(N^2)}$

Efficiency, ${E = S/p}$ = ${O(N^2) / O(N^2)}$ = ${O(1)}$ 

\subsection{Results}
For the testing purpose, synthetic value for large matrix was generated using a program. The randomness of the values is dependent upon the pseudo random function provided by the Java library. In a two node hadoop cluster the map reduce application was ran.\BlankLine

In a $(50,50)$ A and B matrix the run time for map-reduce was approximately 2.5 seconds.

In a $(2000,2000)$ A and B matrix the run time for map-reduce was approximately 6.7 seconds.

It is apparant that in hadoop implementation of map-reduce, the disk I/O is the major bottleneck for the performance.

\section{Decision tree}

\subsection{Problem Overview}
\subsection{Literature Review}
There are not many work related to map-reduce based decision tree algorithm. 
\BlankLine In sequential algorithm of C4.5 uses the Infomation Gain Ratio as the main bias for selecting an attribute for splitting. 

In all literature this has been used but extended in various manner for map-reduce paradigm. In \cite{c452017} the information gain ratio was calculated from a single round of map and reduce function. While in \cite{c45cn} the information gain ratio was reduced from several map-reduce function which generated information gain and split information gain. In \cite {c45in} the information gain ratio is computed from the reduction of the map functions, however they write the information gain to a temporaray space which allows a special function to again compute the best attribute to split.

\cite{c45in} suffers from the fact that in highly distributed manner it might not be the case that the temporary file is avaible to other node, leading to wrong value for information gain ratio selection. Even if limiting the mapper to step forward will increase the parallel time and thus reducing the overall speedup and efficiency of the algorithm. \cite{c45cn} uses a very simple approach, using a pipeline of multiple map and reduce to build up the best attribute selection. Although it is simple, it incurs a lots of disk I/O in current hadoop implementation. However in recent hadoop implementation various optimization of disk I/O should reduce the issue. \cite{c452017} uses a more complicated mapper and reducer to select the best attribute. Compare to other two implementation it is much more optimized in term of number of rounds and disk I/O requirements.

\BlankLine
As the recursion part of the C4.5 to select the subsequent best attribute, recursion of the map-reduce algorithm is done to slowly build up the solution. In \cite{c45cn}, the proposed mechanism used a pipelined version of several map-reduce application, each performing a specific tasks. In \cite{c452017} and \cite{c45in} proposed method do not use pipelined version instead they rely on complex mapper and reducer.

\BlankLine
In the tree growing part for map-reduce C45, \cite{c452017}, \cite{c45cn} and \cite{c45in} uses some variant of map-reduce random forest grow algorithm.

\BlankLine
One major improvement was proposed in \cite{c45cn} by creating three special table to cache the calculation from previous step to the next step. According to their result, they claimed that with this modification they were able to outperform even with single node hadoop performance comparing with sequential algorithm. By using complex mapper and reducer, \cite{c452017} was able to reduce disk I/O overhead and thus increasing the overall algorithm performance. According to their claim, the proposed algorithm scales very well with commodity hardware.

\subsection{Algorithm}
\subsection{Code Snippets}
\subsection{Analysis}
Future direction
Conclusion
Reference

\begin{thebibliography}{9}
\bibitem{wikidt}
\textit https://en.wikipedia.org/wiki/Decision\_tree
\bibitem{hadoopibm}
\textit https://www.ibm.com/developerworks/library/bd-hadoopyarn/
\bibitem{c452017}
Mu, Y., Liu, X., Yang, Z., \& Liu, X. (2017).
\textit A parallel C4. 5 decision tree algorithm based on MapReduce. Concurrency and Computation: Practice and Experience.
\bibitem{c45cn}
Wei Dai, Wei Ji.
\textit A MapReduce Implementation of C4.5 Decision Tree Algorithm.
\bibitem{c45in}
Prayag Surrendran, Redappa G Naidu, Dinesh R. 
\textit Implementation of C4.5 Algorithm Using Hadoop MapReduce Framework.
\bibitem{hadoopap}
Apache Hadoop.
\textit http://hadoop.apache.org/
\end{thebibliography}

http://codingwiththomas.blogspot.ca/2011/04/controlling-hadoop-job-recursion.html
\end{document}
